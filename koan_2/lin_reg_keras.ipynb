{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Keras in TensorFlow\n",
    "\n",
    "This notebook shows how we can train a linear regression using the `tf.keras` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "# Provide short names for useful pieces within keras.\n",
    "regularizers = keras.regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lin_reg_data.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.003),\n",
    "    loss='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 8s on my laptop.\n",
    "history = model.fit(X_train, y_train, epochs=3000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2078271097607083, 0.7459601004918416, 0.7046773897276984]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the history of the mean squared error, sampled\n",
    "# once every 1000 iterations over the training data.\n",
    "history.history['loss'][999::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained w:\n",
      " [[3.4286141]\n",
      " [4.45904  ]]\n",
      "Trained b:\n",
      " [0.8580609]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the learned weights.\n",
    "w, b = model.layers[0].get_weights()\n",
    "print('Trained w:\\n', w)\n",
    "print('Trained b:\\n', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a regularized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "regd_model = keras.Sequential()\n",
    "\n",
    "layer = keras.layers.Dense(\n",
    "    1,\n",
    "    kernel_regularizer = regularizers.l2(alpha)\n",
    "    \n",
    "    # We could have included a bias regularization\n",
    "    # term as well, but that would not match\n",
    "    # the benchmark training from sklearn (which\n",
    "    # doesn't regularize the bias term):\n",
    "    \n",
    "    # bias_regularizer = regularizers.l2(alpha)\n",
    ")\n",
    "\n",
    "regd_model.add(layer)\n",
    "regd_model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.002),\n",
    "    loss='mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 8s on my laptop.\n",
    "history = regd_model.fit(X_train, y_train, epochs=3000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.065326796637642, 12.09166079627143, 12.092484368218315]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the history of the mean squared error, sampled\n",
    "# once every 1000 iterations over the training data.\n",
    "history.history['loss'][999::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained w:\n",
      " [[2.0952854]\n",
      " [3.492356 ]]\n",
      "Trained b:\n",
      " [-4.5394173]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the learned weights.\n",
    "w, b = regd_model.layers[0].get_weights()\n",
    "print('Trained w:\\n', w)\n",
    "print('Trained b:\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkoans-env",
   "language": "python",
   "name": "tfkoans-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
