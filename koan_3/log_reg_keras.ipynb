{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Keras in TensorFlow\n",
    "\n",
    "This notebook shows how we can train a logistic regression using the `tf.keras` interface.\n",
    "\n",
    "TensorFlow comes pre-packages with some imdb movie reviews\n",
    "that are labeled for positive or negative sentament. We'll build a\n",
    "classifier that learns to label new reviews as good or bad.\n",
    "\n",
    "I learned about this dataset and some related ideas from [this\n",
    "tensorflow example](https://github.com/tensorflow/models/blob/master/samples/core/tutorials/keras/basic_text_classification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Print out the TensorFlow version to help others reproduce this notebook.\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "NUM_WORDS  = 1000  # Keep this many words (throw out least popular).\n",
    "INDEX_FROM = 3    # Initial index of kept words.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(\n",
    "    num_words = NUM_WORDS,\n",
    "    index_from = INDEX_FROM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be able to reconstruct English reviews from our numeric data\n",
    "\n",
    "Each review in `X_train` and `X_test` is a list of integers; each integer is\n",
    "assigned to its own word based on an index that `keras` has determined for us,\n",
    "and that we have access to. For the sake of human satisfaction, let's get a\n",
    "sense for what the data is like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create a reverse map, and augment it with symbolic tokens.\n",
    "id_to_word = {(v + INDEX_FROM): k for k, v in word_index.items()}\n",
    "id_to_word[0] = '<PAD>'\n",
    "id_to_word[1] = '<START>'\n",
    "id_to_word[2] = '<UNK>'\n",
    "\n",
    "def rebuild_original_review(word_ids):\n",
    "    \"\"\" Return a string based on the given list of `word_ids`. \"\"\"\n",
    "    return ' '.join(id_to_word[id] for id in word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label: 1\n",
      "Review:\n",
      "<START> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone \n",
      "\n",
      "Label: 0\n",
      "Review:\n",
      "<START> big <UNK> big <UNK> bad music and a <UNK> <UNK> <UNK> these are the words to best <UNK> this terrible movie i love cheesy horror movies and i've seen <UNK> but this had got to be on of the worst ever made the plot is <UNK> <UNK> and ridiculous the acting is an <UNK> the script is completely <UNK> the best is the end <UNK> with the <UNK> and how he worked out who the killer is it's just so <UNK> <UNK> written the <UNK> are <UNK> and funny in <UNK> <UNK> the <UNK> is big lots of <UNK> <UNK\n"
     ]
    }
   ],
   "source": [
    "# Let's see two sample reviews.\n",
    "\n",
    "for i in range(2):\n",
    "    print()\n",
    "    print('Label: %d' % y_train[i])\n",
    "    print('Review:')\n",
    "    print(rebuild_original_review(X_train[i])[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to fixed-length vectors\n",
    "\n",
    "We'll use a bag-of-words model to convert each review to a fixed-length\n",
    "vector, where each coordinate has value 0 or 1 depending on if a word\n",
    "corresponding to that column is absent or present in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bag_of_words(array_of_lists):\n",
    "    \"\"\" Return a 0/1 matrix representing the input `array_of_lists`\n",
    "        using the bag-of-words model. \"\"\"\n",
    "    n_pts = array_of_lists.shape[0]\n",
    "    X = np.zeros((n_pts, NUM_WORDS))\n",
    "    for row_idx, word_ids in enumerate(array_of_lists):\n",
    "        X[row_idx, word_ids] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_to_bag_of_words(X_train)\n",
    "X_test  = convert_to_bag_of_words(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(0.0002),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 1s 33us/step - loss: 0.6195 - acc: 0.6768\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 1s 28us/step - loss: 0.5123 - acc: 0.8032\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 1s 27us/step - loss: 0.4549 - acc: 0.8336\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 1s 28us/step - loss: 0.4190 - acc: 0.8456\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 1s 27us/step - loss: 0.3946 - acc: 0.8536\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.3768 - acc: 0.8572\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 1s 28us/step - loss: 0.3635 - acc: 0.8608\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 1s 34us/step - loss: 0.3532 - acc: 0.8628\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 1s 29us/step - loss: 0.3450 - acc: 0.8652\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 1s 28us/step - loss: 0.3384 - acc: 0.8669\n"
     ]
    }
   ],
   "source": [
    "# This takes about 12s on my laptop.\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      "[0.39455839398384096, 0.3384308640670776]\n"
     ]
    }
   ],
   "source": [
    "print('Losses:')\n",
    "print(history.history['loss'][4::5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies:\n",
      "[0.85364, 0.86692]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracies:')\n",
    "print(history.history['acc'][4::5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the test-set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 14us/step\n",
      "\n",
      "Test accuracy:\n",
      "0.86048\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "\n",
    "print()\n",
    "print('Test accuracy:')\n",
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkoans-env",
   "language": "python",
   "name": "tfkoans-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
